{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (0.10.11)\n",
      "Requirement already satisfied: torch in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (0.18.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of pyodbc: Invalid version: '4.0.0-unsupported'\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.99-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting fer\n",
      "  Downloading fer-22.5.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.13)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from mediapipe) (3.7.5)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from mediapipe) (4.12.0.88)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from torch) (1.6.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from torch) (2.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from torch) (2.11.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: comtypes in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from pyttsx3) (1.1.7)\n",
      "Collecting pypiwin32 (from pyttsx3)\n",
      "  Downloading pypiwin32-223-py3-none-any.whl.metadata (236 bytes)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from pyttsx3) (227)\n",
      "Requirement already satisfied: keras>=2.0.0 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from fer) (2.13.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from fer) (2.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from fer) (2.32.3)\n",
      "Collecting facenet-pytorch (from fer)\n",
      "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from fer) (4.66.4)\n",
      "Collecting moviepy (from fer)\n",
      "  Downloading moviepy-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting ffmpeg==1.4 (from fer)\n",
      "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.14.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->fer) (0.4.4)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-10.2.0-cp38-cp38-win_amd64.whl.metadata (9.9 kB)\n",
      "INFO: pip is looking at multiple versions of facenet-pytorch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting facenet-pytorch (from fer)\n",
      "  Downloading facenet_pytorch-2.5.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from jax->mediapipe) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.10.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from jax->mediapipe) (8.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (6.4.0)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from moviepy->fer) (4.4.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\harshit kumar\\anaconda3\\lib\\site-packages (from moviepy->fer) (2.9.0)\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy->fer)\n",
      "  Downloading imageio_ffmpeg-0.5.1-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "INFO: pip is looking at multiple versions of moviepy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting moviepy (from fer)\n",
      "  Downloading moviepy-2.2.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading moviepy-2.1.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading moviepy-2.1.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading moviepy-2.1.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Downloading moviepy-2.0.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "  Downloading moviepy-1.0.3.tar.gz (388 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python mediapipe torch torchvision torchaudio pyttsx3 fer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model from gesture_lstm_cpu_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:comtypes.client._code_cache:Imported existing <module 'comtypes.gen' from 'C:\\\\Users\\\\harshit kumar\\\\anaconda3\\\\lib\\\\site-packages\\\\comtypes\\\\gen\\\\__init__.py'>\n",
      "INFO:comtypes.client._code_cache:Using writeable comtypes cache directory: 'C:\\Users\\harshit kumar\\anaconda3\\lib\\site-packages\\comtypes\\gen'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "import pyttsx3\n",
    "import torch.nn as nn\n",
    "from fer import FER\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# Settings\n",
    "# -----------------------\n",
    "\n",
    "SEQ_LEN = 20  # number of frames for LSTM gesture\n",
    "GESTURES = [\"Food\", \"I\", \"Sorry\", \"Thank You\", \"Water\"]  # your trained gestures\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "MODEL_PATH = \"gesture_lstm_cpu_2.pth\"\n",
    "\n",
    "# -----------------------\n",
    "# Load Hand Gesture Model (Module 1 LSTM)\n",
    "# -----------------------\n",
    "class GestureLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=126, hidden_dim=64, num_layers=1, num_classes=5, dropout=0.3):\n",
    "        super(GestureLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]     # last timestep\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "device='cpu'\n",
    "\n",
    "def load_lstm_model():\n",
    "    model = GestureLSTM(input_dim=126, hidden_dim=64, num_classes=len(GESTURES)).to(device)\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        model.eval()\n",
    "        print(\"Loaded trained model from\", MODEL_PATH)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{MODEL_PATH} not found. Train model first.\")\n",
    "    return model\n",
    "\n",
    "model_kp = load_lstm_model()\n",
    "\n",
    "# -----------------------\n",
    "# Initialize MediaPipe Hands & FER\n",
    "# -----------------------\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.3, min_tracking_confidence=0.3)\n",
    "buf = collections.deque(maxlen=SEQ_LEN)\n",
    "\n",
    "face_detector = FER(mtcnn=True)\n",
    "\n",
    "# -----------------------\n",
    "# Text-to-Speech\n",
    "# -----------------------\n",
    "engine = pyttsx3.init()\n",
    "last_gesture = None\n",
    "last_emotion = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "import pyttsx3\n",
    "from fer import FER  # pre-trained facial expression recognition\n",
    "\n",
    "# Hand Gesture Setup (reuse Module 1)\n",
    "SEQ_LEN = 20\n",
    "GESTURES = [\"Food\", \"I\", \"Sorry\", \"Thank You\", \"Water\"]\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load LSTM gesture model (Module 1)\n",
    "model_kp = GestureLSTM(input_dim=126, hidden_dim=64, num_classes=len(GESTURES)).to(device)\n",
    "model_kp.load_state_dict(torch.load(\"gesture_lstm_cpu_2.pth\", map_location=device))\n",
    "model_kp.eval()\n",
    "\n",
    "# Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.3, min_tracking_confidence=0.3)\n",
    "buf = collections.deque(maxlen=SEQ_LEN)\n",
    "\n",
    "# FER Detector\n",
    "face_detector = FER(mtcnn=True)\n",
    "\n",
    "# Text-to-Speech\n",
    "#engine = pyttsx3.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:<ipython-input-3-69feca22f94e>:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  seq_input = torch.tensor([list(buf)], dtype=torch.float32).to(DEVICE)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "prev_time = 0\n",
    "buf = collections.deque(maxlen=SEQ_LEN)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # ---- Hand Gesture ----\n",
    "    res = hands.process(rgb)\n",
    "    data = []\n",
    "    hands_detected = 0\n",
    "    if res.multi_hand_landmarks:\n",
    "        hands_detected = len(res.multi_hand_landmarks)\n",
    "        for i, hand in enumerate(res.multi_hand_landmarks):\n",
    "            if i >= 2: break\n",
    "            for lm in hand.landmark:\n",
    "                data += [lm.x, lm.y, lm.z]\n",
    "        while len(data) < 126:\n",
    "            data += [0]\n",
    "        for hand_landmarks in res.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    else:\n",
    "        data = [0]*126\n",
    "\n",
    "    buf.append(np.array(data, dtype=np.float32))\n",
    "\n",
    "    # Predict Gesture only if buffer is full and hands detected\n",
    "    gesture_pred = None\n",
    "    if len(buf) == SEQ_LEN and hands_detected > 0:\n",
    "        seq_input = torch.tensor([list(buf)], dtype=torch.float32).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model_kp(seq_input)\n",
    "            pred = out.argmax(dim=1).item()\n",
    "        gesture_pred = GESTURES[pred]\n",
    "        cv2.putText(frame, f\"Gesture: {gesture_pred}\", (10,40), cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "\n",
    "    # ---- Facial Emotion ----\n",
    "    face_emotions = face_detector.top_emotion(frame)\n",
    "    emotion_label = \"Unknown\"\n",
    "    if face_emotions is not None:\n",
    "        emotion_label = face_emotions[0]\n",
    "        cv2.putText(frame, f\"Emotion: {emotion_label}\", (10,80), cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2)\n",
    "\n",
    "    # ---- FPS display ----\n",
    "    curr_time = time.time()\n",
    "    fps = 1 / (curr_time - prev_time) if prev_time else 0\n",
    "    prev_time = curr_time\n",
    "    cv2.putText(frame, f\"FPS: {int(fps)}\", (10,110), cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,0,255),2)\n",
    "\n",
    "    cv2.imshow(\"Gesture & Emotion Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
