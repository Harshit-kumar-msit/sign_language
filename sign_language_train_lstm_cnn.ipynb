{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (4.10.0.84)\n",
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.21-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (2.8.0)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.23.0-cp312-cp312-win_amd64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.8.0-cp312-cp312-win_amd64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (4.67.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: absl-py in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (24.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (24.3.25)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.7.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (3.9.3)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.7.2-cp312-cp312-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from jax->mediapipe) (0.5.3)\n",
      "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.12 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from jax->mediapipe) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vkr82\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Using cached mediapipe-0.10.21-cp312-cp312-win_amd64.whl (51.0 MB)\n",
      "Using cached torchvision-0.23.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "Using cached torchaudio-2.8.0-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "Using cached jax-0.7.1-py3-none-any.whl (2.8 MB)\n",
      "Installing collected packages: torchvision, torchaudio, jax, mediapipe\n",
      "Successfully installed jax-0.7.1 mediapipe-0.10.21 torchaudio-2.8.0 torchvision-0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\vkr82\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\vkr82\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\vkr82\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\vkr82\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\vkr82\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\vkr82\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Run this in a Jupyter cell with a leading ! or in terminal\n",
    "!pip install opencv-python mediapipe torch torchvision torchaudio tqdm pyttsx3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Single Gesture Recorder\n",
    "# -----------------------------\n",
    "\n",
    "import cv2, os, time\n",
    "\n",
    "# âœ… Change this to the gesture you want to record\n",
    "gesture = \"Water\"  \n",
    "\n",
    "BASE_DIR = \"dataset\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, gesture), exist_ok=True)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 20.0\n",
    "\n",
    "# Count existing recordings\n",
    "count = len(os.listdir(os.path.join(BASE_DIR, gesture)))\n",
    "print(f\"Recording gesture: {gesture}\")\n",
    "print(\"Press 'r' to start recording a ~2s clip. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    cv2.imshow(\"Single Gesture Recorder\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    \n",
    "    # Press 'r' to record\n",
    "    if key == ord('r'):\n",
    "        out_name = f\"{gesture}_{count}.mp4\"\n",
    "        out_path = os.path.join(BASE_DIR, gesture, out_name)\n",
    "        h, w = frame.shape[:2]\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(out_path, fourcc, 20.0, (w, h))\n",
    "        print(f\"Recording {gesture} -> {out_path}\")\n",
    "        start = time.time()\n",
    "        # Record ~2 seconds\n",
    "        while time.time() - start < 2.0:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            writer.write(frame)\n",
    "            cv2.imshow(\"Single Gesture Recorder\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        writer.release()\n",
    "        count += 1\n",
    "        print(f\"Saved {out_path}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed/keypoints\\Food\\Food_5.npy\n",
      "Saved processed/keypoints\\Food\\Food_6.npy\n",
      "Saved processed/keypoints\\I\\I_5.npy\n",
      "Saved processed/keypoints\\I\\I_6.npy\n",
      "Saved processed/keypoints\\Sorry\\Sorry_5.npy\n",
      "Saved processed/keypoints\\Sorry\\Sorry_6.npy\n",
      "Saved processed/keypoints\\ThankYou\\ThankYou_5.npy\n",
      "Saved processed/keypoints\\ThankYou\\ThankYou_6.npy\n",
      "Saved processed/keypoints\\ThankYou\\ThankYou_7.npy\n",
      "Saved processed/keypoints\\Water\\Water_5.npy\n",
      "Saved processed/keypoints\\Water\\Water_6.npy\n",
      "Saved processed/keypoints\\Water\\Water_7.npy\n",
      "Saved processed/keypoints\\Water\\Water_8.npy\n"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np, cv2, mediapipe as mp\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "BASE=\"dataset\"\n",
    "OUT=\"processed/keypoints\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "SEQ_LEN = 20  # smaller seq_len for CPU\n",
    "\n",
    "def extract_keypoints_from_video(path, seq_len=SEQ_LEN):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "    seq=[]\n",
    "    while len(seq) < seq_len:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = hands.process(rgb)\n",
    "        if res.multi_hand_landmarks:\n",
    "            data=[]\n",
    "            for hand in res.multi_hand_landmarks:\n",
    "                for lm in hand.landmark:\n",
    "                    data += [lm.x, lm.y, lm.z]\n",
    "            if len(res.multi_hand_landmarks)==1:\n",
    "                data += [0]*63\n",
    "            seq.append(data)\n",
    "        else:\n",
    "            seq.append([0]*126)\n",
    "    while len(seq) < seq_len:\n",
    "        seq.append([0]*126)\n",
    "    cap.release(); hands.close()\n",
    "    return np.array(seq, dtype=np.float32)\n",
    "\n",
    "for gesture in sorted(os.listdir(BASE)):\n",
    "    in_dir = os.path.join(BASE, gesture)\n",
    "    out_dir = os.path.join(OUT, gesture)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for mp4 in glob.glob(os.path.join(in_dir,\"*.mp4\")):\n",
    "        name = os.path.splitext(os.path.basename(mp4))[0]\n",
    "        out_path = os.path.join(out_dir, name + \".npy\")\n",
    "        if os.path.exists(out_path): continue\n",
    "        arr = extract_keypoints_from_video(mp4)\n",
    "        np.save(out_path, arr)\n",
    "        print(\"Saved\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved frames processed/frames\\Food\\Food_5.npy\n",
      "Saved frames processed/frames\\Food\\Food_6.npy\n",
      "Saved frames processed/frames\\I\\I_5.npy\n",
      "Saved frames processed/frames\\I\\I_6.npy\n",
      "Saved frames processed/frames\\Sorry\\Sorry_5.npy\n",
      "Saved frames processed/frames\\Sorry\\Sorry_6.npy\n",
      "Saved frames processed/frames\\ThankYou\\ThankYou_5.npy\n",
      "Saved frames processed/frames\\ThankYou\\ThankYou_6.npy\n",
      "Saved frames processed/frames\\ThankYou\\ThankYou_7.npy\n",
      "Saved frames processed/frames\\Water\\Water_5.npy\n",
      "Saved frames processed/frames\\Water\\Water_6.npy\n",
      "Saved frames processed/frames\\Water\\Water_7.npy\n",
      "Saved frames processed/frames\\Water\\Water_8.npy\n"
     ]
    }
   ],
   "source": [
    "BASE=\"dataset\"\n",
    "OUT=\"processed/frames\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "TARGET_FRAMES = 8    # fewer frames\n",
    "TARGET_SIZE = (64,64)  # smaller resolution\n",
    "\n",
    "def extract_video_clip(path, target_frames=TARGET_FRAMES, size=TARGET_SIZE):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames=[]\n",
    "    while len(frames) < target_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        frame = cv2.resize(frame, size)\n",
    "        frames.append(frame[..., ::-1])\n",
    "    cap.release()\n",
    "    while len(frames) < target_frames:\n",
    "        frames.append(frames[-1] if frames else np.zeros((size[1],size[0],3), dtype=np.uint8))\n",
    "    arr = np.stack(frames, axis=0).astype(np.uint8)\n",
    "    return arr\n",
    "\n",
    "for gesture in sorted(os.listdir(BASE)):\n",
    "    in_dir = os.path.join(BASE,gesture)\n",
    "    out_dir = os.path.join(OUT,gesture)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for mp4 in glob.glob(os.path.join(in_dir,\"*.mp4\")):\n",
    "        name = os.path.splitext(os.path.basename(mp4))[0]\n",
    "        out_path = os.path.join(out_dir, name + \".npy\")\n",
    "        if os.path.exists(out_path): continue\n",
    "        clip = extract_video_clip(mp4)\n",
    "        np.save(out_path, clip)\n",
    "        print(\"Saved frames\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# CPU only\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch.utils'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, os, glob\n",
    "\n",
    "device = torch.device(\"cpu\")  # CPU only\n",
    "\n",
    "# Keypoint dataset\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, root=\"processed/keypoints\"):\n",
    "        self.samples=[]; self.labels=[]\n",
    "        self.classes = sorted(os.listdir(root))\n",
    "        for i,cls in enumerate(self.classes):\n",
    "            for npy in glob.glob(os.path.join(root,cls,\"*.npy\")):\n",
    "                self.samples.append(npy)\n",
    "                self.labels.append(i)\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        arr = np.load(self.samples[idx])\n",
    "        return torch.tensor(arr, dtype=torch.float32), self.labels[idx]\n",
    "\n",
    "# Frame dataset\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root=\"processed/frames\", seq_len=12, resize=(32,32)):\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.seq_len = seq_len\n",
    "        self.resize = resize\n",
    "        self.classes = sorted(os.listdir(root))\n",
    "        self.class2idx = {c:i for i,c in enumerate(self.classes)}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(root, cls)\n",
    "            for npy in glob.glob(os.path.join(cls_path, \"*.npy\")):\n",
    "                self.samples.append(npy)\n",
    "                self.labels.append(self.class2idx[cls])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        arr = np.load(self.samples[idx]).astype(np.float32)/255.0  # normalize\n",
    "        # Pad or crop sequence to seq_len\n",
    "        T, H, W, C = arr.shape\n",
    "        if T < self.seq_len:\n",
    "            pad = np.zeros((self.seq_len - T, H, W, C), dtype=np.float32)\n",
    "            arr = np.concatenate([arr, pad], axis=0)\n",
    "        elif T > self.seq_len:\n",
    "            arr = arr[:self.seq_len]\n",
    "\n",
    "        # Resize frames if needed\n",
    "        if self.resize:\n",
    "            arr_resized = np.stack([cv2.resize(f, self.resize) for f in arr], axis=0)\n",
    "            arr = arr_resized\n",
    "\n",
    "        # Transpose to [C, T, H, W] for Conv3D\n",
    "        arr = np.transpose(arr, (3,0,1,2))\n",
    "        return torch.tensor(arr, dtype=torch.float32), self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.nn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGestureLSTM\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m126\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch.nn'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GestureLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=126, hidden_dim=64, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "    def forward(self,x):\n",
    "        out,_ = self.lstm(x)\n",
    "        return self.fc(out[:,-1,:])\n",
    "\n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv3d(3,16,3,padding=1), nn.ReLU(), nn.MaxPool3d((1,2,2)),\n",
    "            nn.Conv3d(16,32,3,padding=1), nn.ReLU(), nn.MaxPool3d((2,2,2)),\n",
    "            nn.Conv3d(32,64,3,padding=1), nn.ReLU(), nn.AdaptiveAvgPool3d((1,1,1)),\n",
    "        )\n",
    "        self.fc = nn.Linear(64,num_classes)\n",
    "    def forward(self,x):\n",
    "        x=self.net(x)\n",
    "        x=x.view(x.size(0),-1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace with your file path\n",
    "file_path = \"processed/keypoints/I/I_0.npy\"\n",
    "\n",
    "data = np.load(file_path)\n",
    "print(data.shape)   # Check the shape of the array\n",
    "print(data)         # See the actual data\n",
    "print(\"Sequence length:\", data.shape[0])\n",
    "print(\"Number of features per frame:\", data.shape[1])\n",
    "print(\"First frame keypoints:\", data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "kp_dataset = KeypointDataset(\"processed/keypoints\")\n",
    "kp_loader = DataLoader(kp_dataset, batch_size=4, shuffle=True)  # small batch\n",
    "\n",
    "model_kp = GestureLSTM(input_dim=kp_dataset[0][0].shape[1], hidden_dim=64, num_classes=len(kp_dataset.classes)).to(device)\n",
    "opt = torch.optim.Adam(model_kp.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for ep in range(50):  # fewer epochs for CPU\n",
    "    total_loss=0; cnt=0\n",
    "    model_kp.train()\n",
    "    for X,y in kp_loader:\n",
    "        X=X.to(device); y=torch.tensor(y).to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model_kp(X)\n",
    "        loss = crit(out,y)\n",
    "        loss.backward(); opt.step()\n",
    "        total_loss+=loss.item(); cnt+=1\n",
    "    print(f\"Epoch {ep+1}, loss={total_loss/cnt:.4f}\")\n",
    "torch.save(model_kp.state_dict(),\"gesture_lstm_cpu.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# --- Dataset ---\n",
    "# Make sure your VideoDataset class accepts seq_len and resize\n",
    "video_ds = VideoDataset(\"processed/frames\", seq_len=12, resize=(32,32))\n",
    "video_loader = DataLoader(video_ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# --- Model ---\n",
    "model_3d = Simple3DCNN(num_classes=len(video_ds.classes)).to(device)\n",
    "opt3 = torch.optim.Adam(model_3d.parameters(), lr=1e-4)\n",
    "crit3 = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Training parameters ---\n",
    "num_epochs = 50  # can increase further for better accuracy\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    model_3d.train()\n",
    "    \n",
    "    for X, y in video_loader:\n",
    "        # Ensure correct tensor type\n",
    "        X = X.float().to(device)       # [batch, C, seq_len, H, W]\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        \n",
    "        opt3.zero_grad()\n",
    "        out = model_3d(X)\n",
    "        loss = crit3(out, y)\n",
    "        loss.backward()\n",
    "        opt3.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        cnt += 1\n",
    "    \n",
    "    print(f\"3D Epoch {ep+1}/{num_epochs}, loss={total_loss/cnt:.4f}\")\n",
    "\n",
    "# --- Save model ---\n",
    "MODEL_SAVE_PATH = \"gesture_3d_cpu.pth\"\n",
    "torch.save(model_3d.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Saved improved 3D-CNN model to {MODEL_SAVE_PATH}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "def eval_model(model, dataset, batch_size=1):\n",
    "    model.eval()\n",
    "    preds=[]; trues=[]\n",
    "    for X,y in DataLoader(dataset,batch_size=batch_size):\n",
    "        X=X.to(device)\n",
    "        with torch.no_grad(): out = model(X)\n",
    "        preds += out.argmax(1).cpu().numpy().tolist()\n",
    "        trues += y\n",
    "    return accuracy_score(trues,preds)\n",
    "\n",
    "acc_kp = eval_model(model_kp,kp_dataset)\n",
    "acc_3d = eval_model(model_3d,video_ds)\n",
    "print(\"LSTM CPU acc:\",acc_kp)\n",
    "print(\"3D-CNN CPU acc:\",acc_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyttsx3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyttsx3\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Real-time LSTM Gesture Recognition (CPU)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     13\u001b[0m mp_hands \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mhands\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyttsx3'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import cv2, mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pyttsx3\n",
    "\n",
    "# -----------------------\n",
    "# Real-time LSTM Gesture Recognition (CPU)\n",
    "# -----------------------\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "SEQ_LEN = 20  # should match training\n",
    "GESTURES = [\"Food\", \"I\", \"Sorry\", \"Thank You\", \"Water\"]  # same as training\n",
    "device = torch.device(\"cpu\")\n",
    "MODEL_PATH = \"gesture_lstm_cpu.pth\"\n",
    "\n",
    "# Function to load model\n",
    "def load_lstm_model():\n",
    "    model = GestureLSTM(input_dim=126, hidden_dim=64, num_classes=len(GESTURES)).to(device)\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        model.eval()\n",
    "        print(\"Loaded trained model from\", MODEL_PATH)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{MODEL_PATH} not found. Train model first.\")\n",
    "    return model\n",
    "\n",
    "# Load model once\n",
    "model_kp = load_lstm_model()\n",
    "\n",
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.3,  # lower threshold\n",
    "    min_tracking_confidence=0.3\n",
    ")\n",
    "buf = collections.deque(maxlen=SEQ_LEN)\n",
    "\n",
    "prev_time = 0  # for FPS calculation\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    res = hands.process(rgb)\n",
    "    \n",
    "    # Extract keypoints for LSTM\n",
    "    data = []\n",
    "    hands_detected = 0\n",
    "    if res.multi_hand_landmarks:\n",
    "        hands_detected = len(res.multi_hand_landmarks)\n",
    "        for i, hand in enumerate(res.multi_hand_landmarks):\n",
    "            if i >= 2:\n",
    "                break\n",
    "            for lm in hand.landmark:\n",
    "                data += [lm.x, lm.y, lm.z]\n",
    "        while len(data) < 126:\n",
    "            data += [0]\n",
    "        for hand_landmarks in res.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    else:\n",
    "        data = [0]*126\n",
    "\n",
    "    buf.append(np.array(data, dtype=np.float32))\n",
    "\n",
    "    # Show buffer status\n",
    "    cv2.putText(frame, f\"Buffer: {len(buf)}/{SEQ_LEN}\", (10, 110),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "    # Show warning if no hands\n",
    "    if hands_detected == 0:\n",
    "        cv2.putText(frame, \"No hands detected!\", (10, 140),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # Predict gesture only if at least one hand is detected\n",
    "    if len(buf) == SEQ_LEN and hands_detected > 0:\n",
    "        seq_input = torch.tensor([list(buf)], dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model_kp(seq_input)\n",
    "            pred = out.argmax(dim=1).item()\n",
    "        gesture_word = GESTURES[pred]    \n",
    "        cv2.putText(frame, f\"Gesture: {GESTURES[pred]}\", (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        print(f\"[DEBUG] Hands detected: {hands_detected}, Predicted gesture: {GESTURES[pred]}\")\n",
    "        tts_engine.say(gesture_word)\n",
    "        tts_engine.runAndWait()\n",
    "\n",
    "    # Calculate and display FPS\n",
    "    curr_time = time.time()\n",
    "    fps = 1 / (curr_time - prev_time) if prev_time else 0\n",
    "    prev_time = curr_time\n",
    "    cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 80),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Real-time Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "# ...existing code...\n",
    "model_3d = Simple3DCNN(num_classes=len(GESTURES)).to(device)\n",
    "model_3d.load_state_dict(torch.load(\"gesture_3d_cpu.pth\", map_location=device))\n",
    "model_3d.eval()\n",
    "print(\"Loaded 3D-CNN model successfully!\")\n",
    "\n",
    "tts_engine = pyttsx3.init()  # Add TTS engine\n",
    "\n",
    "SEQ_LEN = 16  # match what was used in training\n",
    "frame_buffer = deque(maxlen=SEQ_LEN)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize to 64x64 (or your training size)\n",
    "    frame_resized = cv2.resize(frame, (64,64))\n",
    "    frame_buffer.append(frame_resized)\n",
    "\n",
    "    # Predict when buffer is full\n",
    "    if len(frame_buffer) == SEQ_LEN:\n",
    "        # Convert buffer to tensor: [batch=1, C=3, seq, H, W]\n",
    "        frames_np = np.stack(frame_buffer, axis=0)       # [seq, H, W, C]\n",
    "        frames_np = frames_np.transpose(3,0,1,2)        # [C, seq, H, W]\n",
    "        frames_tensor = torch.tensor(frames_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        frames_tensor /= 255.0  # normalize if model trained on [0,1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model_3d(frames_tensor)\n",
    "            pred = out.argmax(dim=1).item()\n",
    "        gesture_word = GESTURES[pred]\n",
    "        cv2.putText(frame, f\"3D-CNN Gesture: {gesture_word}\", (10,40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "        tts_engine.say(gesture_word)\n",
    "        tts_engine.runAndWait()\n",
    "\n",
    "    cv2.imshow(\"3D-CNN Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
