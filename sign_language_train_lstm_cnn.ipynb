{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Run this in a Jupyter cell with a leading ! or in terminal\n",
    "!pip install opencv-python mediapipe torch torchvision torchaudio tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording gesture: Food\n",
      "Press 'r' to start recording a ~2s clip. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Single Gesture Recorder\n",
    "# -----------------------------\n",
    "\n",
    "import cv2, os, time\n",
    "\n",
    "# âœ… Change this to the gesture you want to record\n",
    "gesture = \"Food\"  \n",
    "\n",
    "BASE_DIR = \"dataset\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, gesture), exist_ok=True)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 20.0\n",
    "\n",
    "# Count existing recordings\n",
    "count = len(os.listdir(os.path.join(BASE_DIR, gesture)))\n",
    "print(f\"Recording gesture: {gesture}\")\n",
    "print(\"Press 'r' to start recording a ~2s clip. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    cv2.imshow(\"Single Gesture Recorder\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    \n",
    "    # Press 'r' to record\n",
    "    if key == ord('r'):\n",
    "        out_name = f\"{gesture}_{count}.mp4\"\n",
    "        out_path = os.path.join(BASE_DIR, gesture, out_name)\n",
    "        h, w = frame.shape[:2]\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(out_path, fourcc, 20.0, (w, h))\n",
    "        print(f\"Recording {gesture} -> {out_path}\")\n",
    "        start = time.time()\n",
    "        # Record ~2 seconds\n",
    "        while time.time() - start < 2.0:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            writer.write(frame)\n",
    "            cv2.imshow(\"Single Gesture Recorder\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        writer.release()\n",
    "        count += 1\n",
    "        print(f\"Saved {out_path}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mglob\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mcv2\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m mp_hands \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mhands\n\u001b[0;32m      4\u001b[0m BASE\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import os, glob, numpy as np, cv2, mediapipe as mp\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "BASE=\"dataset\"\n",
    "OUT=\"processed/keypoints\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "SEQ_LEN = 20  # smaller seq_len for CPU\n",
    "\n",
    "def extract_keypoints_from_video(path, seq_len=SEQ_LEN):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "    seq=[]\n",
    "    while len(seq) < seq_len:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = hands.process(rgb)\n",
    "        if res.multi_hand_landmarks:\n",
    "            data=[]\n",
    "            for hand in res.multi_hand_landmarks:\n",
    "                for lm in hand.landmark:\n",
    "                    data += [lm.x, lm.y, lm.z]\n",
    "            if len(res.multi_hand_landmarks)==1:\n",
    "                data += [0]*63\n",
    "            seq.append(data)\n",
    "        else:\n",
    "            seq.append([0]*126)\n",
    "    while len(seq) < seq_len:\n",
    "        seq.append([0]*126)\n",
    "    cap.release(); hands.close()\n",
    "    return np.array(seq, dtype=np.float32)\n",
    "\n",
    "for gesture in sorted(os.listdir(BASE)):\n",
    "    in_dir = os.path.join(BASE, gesture)\n",
    "    out_dir = os.path.join(OUT, gesture)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for mp4 in glob.glob(os.path.join(in_dir,\"*.mp4\")):\n",
    "        name = os.path.splitext(os.path.basename(mp4))[0]\n",
    "        out_path = os.path.join(out_dir, name + \".npy\")\n",
    "        if os.path.exists(out_path): continue\n",
    "        arr = extract_keypoints_from_video(mp4)\n",
    "        np.save(out_path, arr)\n",
    "        print(\"Saved\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE=\"dataset\"\n",
    "OUT=\"processed/frames\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "TARGET_FRAMES = 8    # fewer frames\n",
    "TARGET_SIZE = (64,64)  # smaller resolution\n",
    "\n",
    "def extract_video_clip(path, target_frames=TARGET_FRAMES, size=TARGET_SIZE):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames=[]\n",
    "    while len(frames) < target_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        frame = cv2.resize(frame, size)\n",
    "        frames.append(frame[..., ::-1])\n",
    "    cap.release()\n",
    "    while len(frames) < target_frames:\n",
    "        frames.append(frames[-1] if frames else np.zeros((size[1],size[0],3), dtype=np.uint8))\n",
    "    arr = np.stack(frames, axis=0).astype(np.uint8)\n",
    "    return arr\n",
    "\n",
    "for gesture in sorted(os.listdir(BASE)):\n",
    "    in_dir = os.path.join(BASE,gesture)\n",
    "    out_dir = os.path.join(OUT,gesture)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for mp4 in glob.glob(os.path.join(in_dir,\"*.mp4\")):\n",
    "        name = os.path.splitext(os.path.basename(mp4))[0]\n",
    "        out_path = os.path.join(out_dir, name + \".npy\")\n",
    "        if os.path.exists(out_path): continue\n",
    "        clip = extract_video_clip(mp4)\n",
    "        np.save(out_path, clip)\n",
    "        print(\"Saved frames\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, os, glob\n",
    "\n",
    "device = torch.device(\"cpu\")  # CPU only\n",
    "\n",
    "# Keypoint dataset\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, root=\"processed/keypoints\"):\n",
    "        self.samples=[]; self.labels=[]\n",
    "        self.classes = sorted(os.listdir(root))\n",
    "        for i,cls in enumerate(self.classes):\n",
    "            for npy in glob.glob(os.path.join(root,cls,\"*.npy\")):\n",
    "                self.samples.append(npy)\n",
    "                self.labels.append(i)\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        arr = np.load(self.samples[idx])\n",
    "        return torch.tensor(arr, dtype=torch.float32), self.labels[idx]\n",
    "\n",
    "# Frame dataset\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root=\"processed/frames\", seq_len=12, resize=(32,32)):\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.seq_len = seq_len\n",
    "        self.resize = resize\n",
    "        self.classes = sorted(os.listdir(root))\n",
    "        self.class2idx = {c:i for i,c in enumerate(self.classes)}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(root, cls)\n",
    "            for npy in glob.glob(os.path.join(cls_path, \"*.npy\")):\n",
    "                self.samples.append(npy)\n",
    "                self.labels.append(self.class2idx[cls])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        arr = np.load(self.samples[idx]).astype(np.float32)/255.0  # normalize\n",
    "        # Pad or crop sequence to seq_len\n",
    "        T, H, W, C = arr.shape\n",
    "        if T < self.seq_len:\n",
    "            pad = np.zeros((self.seq_len - T, H, W, C), dtype=np.float32)\n",
    "            arr = np.concatenate([arr, pad], axis=0)\n",
    "        elif T > self.seq_len:\n",
    "            arr = arr[:self.seq_len]\n",
    "\n",
    "        # Resize frames if needed\n",
    "        if self.resize:\n",
    "            arr_resized = np.stack([cv2.resize(f, self.resize) for f in arr], axis=0)\n",
    "            arr = arr_resized\n",
    "\n",
    "        # Transpose to [C, T, H, W] for Conv3D\n",
    "        arr = np.transpose(arr, (3,0,1,2))\n",
    "        return torch.tensor(arr, dtype=torch.float32), self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GestureLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=126, hidden_dim=64, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "    def forward(self,x):\n",
    "        out,_ = self.lstm(x)\n",
    "        return self.fc(out[:,-1,:])\n",
    "\n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv3d(3,16,3,padding=1), nn.ReLU(), nn.MaxPool3d((1,2,2)),\n",
    "            nn.Conv3d(16,32,3,padding=1), nn.ReLU(), nn.MaxPool3d((2,2,2)),\n",
    "            nn.Conv3d(32,64,3,padding=1), nn.ReLU(), nn.AdaptiveAvgPool3d((1,1,1)),\n",
    "        )\n",
    "        self.fc = nn.Linear(64,num_classes)\n",
    "    def forward(self,x):\n",
    "        x=self.net(x)\n",
    "        x=x.view(x.size(0),-1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 126)\n",
      "[[ 2.4861056e-01  8.0554658e-01 -3.3692828e-07 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 2.4820775e-01  8.1114340e-01 -2.3204822e-07 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 2.5373021e-01  8.0301517e-01 -1.8159930e-07 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " ...\n",
      " [ 2.5475645e-01  8.0970347e-01 -1.9008185e-07 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 2.5421721e-01  8.0852681e-01 -1.9612118e-07 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 2.5526619e-01  8.0850506e-01 -1.9354437e-07 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n",
      "Sequence length: 20\n",
      "Number of features per frame: 126\n",
      "First frame keypoints: [ 2.4861056e-01  8.0554658e-01 -3.3692828e-07  2.8120226e-01\n",
      "  7.4818468e-01 -9.8371888e-03  3.4173894e-01  6.8138480e-01\n",
      " -2.4740525e-02  3.7245038e-01  6.2844676e-01 -3.9110396e-02\n",
      "  3.7172490e-01  5.7462859e-01 -5.1273063e-02  3.6721712e-01\n",
      "  6.9618785e-01 -4.4583961e-02  4.3892136e-01  7.6718014e-01\n",
      " -7.1330883e-02  4.1513315e-01  7.7581286e-01 -8.2560070e-02\n",
      "  3.8684571e-01  7.6367956e-01 -8.7380677e-02  3.5119221e-01\n",
      "  7.4575931e-01 -5.2833069e-02  4.2597508e-01  8.1193691e-01\n",
      " -7.2696276e-02  3.9961460e-01  8.1329727e-01 -6.9289714e-02\n",
      "  3.7061006e-01  7.9829091e-01 -6.7240141e-02  3.3600345e-01\n",
      "  7.9845178e-01 -6.1461877e-02  4.0833643e-01  8.5295701e-01\n",
      " -7.6215625e-02  3.8086638e-01  8.5118175e-01 -6.1947636e-02\n",
      "  3.5288239e-01  8.3528048e-01 -5.2899335e-02  3.2247922e-01\n",
      "  8.5052657e-01 -7.0720322e-02  3.8306031e-01  8.8828605e-01\n",
      " -7.7205189e-02  3.5760087e-01  8.8622403e-01 -6.4607278e-02\n",
      "  3.3003557e-01  8.7064815e-01 -5.6487203e-02  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace with your file path\n",
    "file_path = \"processed/keypoints/I/I_0.npy\"\n",
    "\n",
    "data = np.load(file_path)\n",
    "print(data.shape)   # Check the shape of the array\n",
    "print(data)         # See the actual data\n",
    "print(\"Sequence length:\", data.shape[0])\n",
    "print(\"Number of features per frame:\", data.shape[1])\n",
    "print(\"First frame keypoints:\", data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-cad52578b364>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X=X.to(device); y=torch.tensor(y).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss=1.6170\n",
      "Epoch 2, loss=1.5220\n",
      "Epoch 3, loss=1.4046\n",
      "Epoch 4, loss=1.2508\n",
      "Epoch 5, loss=1.1775\n",
      "Epoch 6, loss=1.0120\n",
      "Epoch 7, loss=0.8838\n",
      "Epoch 8, loss=0.7032\n",
      "Epoch 9, loss=0.6312\n",
      "Epoch 10, loss=0.4929\n",
      "Epoch 11, loss=0.5465\n",
      "Epoch 12, loss=0.4824\n",
      "Epoch 13, loss=0.3219\n",
      "Epoch 14, loss=0.3543\n",
      "Epoch 15, loss=0.3159\n",
      "Epoch 16, loss=0.3874\n",
      "Epoch 17, loss=0.4445\n",
      "Epoch 18, loss=0.8801\n",
      "Epoch 19, loss=0.7414\n",
      "Epoch 20, loss=0.6433\n",
      "Epoch 21, loss=0.5932\n",
      "Epoch 22, loss=0.4717\n",
      "Epoch 23, loss=0.5261\n",
      "Epoch 24, loss=0.4635\n",
      "Epoch 25, loss=0.4068\n",
      "Epoch 26, loss=0.5376\n",
      "Epoch 27, loss=0.9858\n",
      "Epoch 28, loss=0.7132\n",
      "Epoch 29, loss=0.6025\n",
      "Epoch 30, loss=0.4785\n",
      "Epoch 31, loss=0.5752\n",
      "Epoch 32, loss=0.5060\n",
      "Epoch 33, loss=0.5015\n",
      "Epoch 34, loss=0.5945\n",
      "Epoch 35, loss=0.4770\n",
      "Epoch 36, loss=0.3914\n",
      "Epoch 37, loss=0.4464\n",
      "Epoch 38, loss=0.9231\n",
      "Epoch 39, loss=0.8353\n",
      "Epoch 40, loss=0.4872\n",
      "Epoch 41, loss=0.4419\n",
      "Epoch 42, loss=0.4213\n",
      "Epoch 43, loss=0.3953\n",
      "Epoch 44, loss=0.3248\n",
      "Epoch 45, loss=0.2748\n",
      "Epoch 46, loss=0.2208\n",
      "Epoch 47, loss=0.1844\n",
      "Epoch 48, loss=0.1697\n",
      "Epoch 49, loss=0.1752\n",
      "Epoch 50, loss=0.1367\n"
     ]
    }
   ],
   "source": [
    "kp_dataset = KeypointDataset(\"processed/keypoints\")\n",
    "kp_loader = DataLoader(kp_dataset, batch_size=4, shuffle=True)  # small batch\n",
    "\n",
    "model_kp = GestureLSTM(input_dim=kp_dataset[0][0].shape[1], hidden_dim=64, num_classes=len(kp_dataset.classes)).to(device)\n",
    "opt = torch.optim.Adam(model_kp.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for ep in range(50):  # fewer epochs for CPU\n",
    "    total_loss=0; cnt=0\n",
    "    model_kp.train()\n",
    "    for X,y in kp_loader:\n",
    "        X=X.to(device); y=torch.tensor(y).to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model_kp(X)\n",
    "        loss = crit(out,y)\n",
    "        loss.backward(); opt.step()\n",
    "        total_loss+=loss.item(); cnt+=1\n",
    "    print(f\"Epoch {ep+1}, loss={total_loss/cnt:.4f}\")\n",
    "torch.save(model_kp.state_dict(),\"gesture_lstm_cpu.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-0658f18108e8>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Epoch 1/50, loss=1.6165\n",
      "3D Epoch 2/50, loss=1.6132\n",
      "3D Epoch 3/50, loss=1.6164\n",
      "3D Epoch 4/50, loss=1.6159\n",
      "3D Epoch 5/50, loss=1.6147\n",
      "3D Epoch 6/50, loss=1.6059\n",
      "3D Epoch 7/50, loss=1.6103\n",
      "3D Epoch 8/50, loss=1.6043\n",
      "3D Epoch 9/50, loss=1.6092\n",
      "3D Epoch 10/50, loss=1.6116\n",
      "3D Epoch 11/50, loss=1.6106\n",
      "3D Epoch 12/50, loss=1.6097\n",
      "3D Epoch 13/50, loss=1.6084\n",
      "3D Epoch 14/50, loss=1.6067\n",
      "3D Epoch 15/50, loss=1.6036\n",
      "3D Epoch 16/50, loss=1.6087\n",
      "3D Epoch 17/50, loss=1.6076\n",
      "3D Epoch 18/50, loss=1.6021\n",
      "3D Epoch 19/50, loss=1.6077\n",
      "3D Epoch 20/50, loss=1.6107\n",
      "3D Epoch 21/50, loss=1.6086\n",
      "3D Epoch 22/50, loss=1.6051\n",
      "3D Epoch 23/50, loss=1.6067\n",
      "3D Epoch 24/50, loss=1.6044\n",
      "3D Epoch 25/50, loss=1.6026\n",
      "3D Epoch 26/50, loss=1.6034\n",
      "3D Epoch 27/50, loss=1.6007\n",
      "3D Epoch 28/50, loss=1.6051\n",
      "3D Epoch 29/50, loss=1.6017\n",
      "3D Epoch 30/50, loss=1.6038\n",
      "3D Epoch 31/50, loss=1.5980\n",
      "3D Epoch 32/50, loss=1.5951\n",
      "3D Epoch 33/50, loss=1.5931\n",
      "3D Epoch 34/50, loss=1.6008\n",
      "3D Epoch 35/50, loss=1.5983\n",
      "3D Epoch 36/50, loss=1.5951\n",
      "3D Epoch 37/50, loss=1.5881\n",
      "3D Epoch 38/50, loss=1.5909\n",
      "3D Epoch 39/50, loss=1.5785\n",
      "3D Epoch 40/50, loss=1.5980\n",
      "3D Epoch 41/50, loss=1.5857\n",
      "3D Epoch 42/50, loss=1.5867\n",
      "3D Epoch 43/50, loss=1.5756\n",
      "3D Epoch 44/50, loss=1.5845\n",
      "3D Epoch 45/50, loss=1.5825\n",
      "3D Epoch 46/50, loss=1.5700\n",
      "3D Epoch 47/50, loss=1.5798\n",
      "3D Epoch 48/50, loss=1.5628\n",
      "3D Epoch 49/50, loss=1.5561\n",
      "3D Epoch 50/50, loss=1.5737\n",
      "Saved improved 3D-CNN model to gesture_3d_cpu.pth!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# --- Dataset ---\n",
    "# Make sure your VideoDataset class accepts seq_len and resize\n",
    "video_ds = VideoDataset(\"processed/frames\", seq_len=12, resize=(32,32))\n",
    "video_loader = DataLoader(video_ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# --- Model ---\n",
    "model_3d = Simple3DCNN(num_classes=len(video_ds.classes)).to(device)\n",
    "opt3 = torch.optim.Adam(model_3d.parameters(), lr=1e-4)\n",
    "crit3 = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Training parameters ---\n",
    "num_epochs = 50  # can increase further for better accuracy\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    model_3d.train()\n",
    "    \n",
    "    for X, y in video_loader:\n",
    "        # Ensure correct tensor type\n",
    "        X = X.float().to(device)       # [batch, C, seq_len, H, W]\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        \n",
    "        opt3.zero_grad()\n",
    "        out = model_3d(X)\n",
    "        loss = crit3(out, y)\n",
    "        loss.backward()\n",
    "        opt3.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        cnt += 1\n",
    "    \n",
    "    print(f\"3D Epoch {ep+1}/{num_epochs}, loss={total_loss/cnt:.4f}\")\n",
    "\n",
    "# --- Save model ---\n",
    "MODEL_SAVE_PATH = \"gesture_3d_cpu.pth\"\n",
    "torch.save(model_3d.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Saved improved 3D-CNN model to {MODEL_SAVE_PATH}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM CPU acc: 1.0\n",
      "3D-CNN CPU acc: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "def eval_model(model, dataset, batch_size=1):\n",
    "    model.eval()\n",
    "    preds=[]; trues=[]\n",
    "    for X,y in DataLoader(dataset,batch_size=batch_size):\n",
    "        X=X.to(device)\n",
    "        with torch.no_grad(): out = model(X)\n",
    "        preds += out.argmax(1).cpu().numpy().tolist()\n",
    "        trues += y\n",
    "    return accuracy_score(trues,preds)\n",
    "\n",
    "acc_kp = eval_model(model_kp,kp_dataset)\n",
    "acc_3d = eval_model(model_3d,video_ds)\n",
    "print(\"LSTM CPU acc:\",acc_kp)\n",
    "print(\"3D-CNN CPU acc:\",acc_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model from gesture_lstm_cpu.pth\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import cv2, mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# Real-time LSTM Gesture Recognition (CPU)\n",
    "# -----------------------\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "SEQ_LEN = 20  # should match training\n",
    "GESTURES = [\"Food\", \"I\", \"Sorry\", \"Thank You\", \"Water\"]  # same as training\n",
    "device = torch.device(\"cpu\")\n",
    "MODEL_PATH = \"gesture_lstm_cpu.pth\"\n",
    "\n",
    "# Function to load model\n",
    "def load_lstm_model():\n",
    "    model = GestureLSTM(input_dim=126, hidden_dim=64, num_classes=len(GESTURES)).to(device)\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        model.eval()\n",
    "        print(\"Loaded trained model from\", MODEL_PATH)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{MODEL_PATH} not found. Train model first.\")\n",
    "    return model\n",
    "\n",
    "# Load model once\n",
    "model_kp = load_lstm_model()\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.3,  # lower threshold\n",
    "    min_tracking_confidence=0.3\n",
    ")\n",
    "buf = collections.deque(maxlen=SEQ_LEN)\n",
    "\n",
    "prev_time = 0  # for FPS calculation\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    res = hands.process(rgb)\n",
    "    \n",
    "    # Extract keypoints for LSTM\n",
    "    data = []\n",
    "    hands_detected = 0\n",
    "    if res.multi_hand_landmarks:\n",
    "        hands_detected = len(res.multi_hand_landmarks)\n",
    "        for i, hand in enumerate(res.multi_hand_landmarks):\n",
    "            if i >= 2:\n",
    "                break\n",
    "            for lm in hand.landmark:\n",
    "                data += [lm.x, lm.y, lm.z]\n",
    "        while len(data) < 126:\n",
    "            data += [0]\n",
    "        for hand_landmarks in res.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    else:\n",
    "        data = [0]*126\n",
    "\n",
    "    buf.append(np.array(data, dtype=np.float32))\n",
    "\n",
    "    # Show buffer status\n",
    "    cv2.putText(frame, f\"Buffer: {len(buf)}/{SEQ_LEN}\", (10, 110),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "    # Show warning if no hands\n",
    "    if hands_detected == 0:\n",
    "        cv2.putText(frame, \"No hands detected!\", (10, 140),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # Predict gesture only if at least one hand is detected\n",
    "    if len(buf) == SEQ_LEN and hands_detected > 0:\n",
    "        seq_input = torch.tensor([list(buf)], dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model_kp(seq_input)\n",
    "            pred = out.argmax(dim=1).item()\n",
    "        cv2.putText(frame, f\"Gesture: {GESTURES[pred]}\", (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        print(f\"[DEBUG] Hands detected: {hands_detected}, Predicted gesture: {GESTURES[pred]}\")\n",
    "\n",
    "    # Calculate and display FPS\n",
    "    curr_time = time.time()\n",
    "    fps = 1 / (curr_time - prev_time) if prev_time else 0\n",
    "    prev_time = curr_time\n",
    "    cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 80),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Real-time Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3D-CNN model successfully!\n"
     ]
    }
   ],
   "source": [
    "model_3d = Simple3DCNN(num_classes=len(GESTURES)).to(device)\n",
    "model_3d.load_state_dict(torch.load(\"gesture_3d_cpu.pth\", map_location=device))\n",
    "model_3d.eval()\n",
    "print(\"Loaded 3D-CNN model successfully!\")\n",
    "\n",
    "SEQ_LEN = 16  # match what was used in training\n",
    "frame_buffer = deque(maxlen=SEQ_LEN)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize to 64x64 (or your training size)\n",
    "    frame_resized = cv2.resize(frame, (64,64))\n",
    "    frame_buffer.append(frame_resized)\n",
    "\n",
    "    # Predict when buffer is full\n",
    "    if len(frame_buffer) == SEQ_LEN:\n",
    "        # Convert buffer to tensor: [batch=1, C=3, seq, H, W]\n",
    "        frames_np = np.stack(frame_buffer, axis=0)       # [seq, H, W, C]\n",
    "        frames_np = frames_np.transpose(3,0,1,2)        # [C, seq, H, W]\n",
    "        frames_tensor = torch.tensor(frames_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        frames_tensor /= 255.0  # normalize if model trained on [0,1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model_3d(frames_tensor)\n",
    "            pred = out.argmax(dim=1).item()\n",
    "        cv2.putText(frame, f\"3D-CNN Gesture: {GESTURES[pred]}\", (10,40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "\n",
    "    cv2.imshow(\"3D-CNN Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
