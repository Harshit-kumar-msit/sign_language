{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Run this in a Jupyter cell with a leading ! or in terminal\n",
    "!pip install opencv-python mediapipe torch torchvision torchaudio tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Single Gesture Recorder\n",
    "# -----------------------------\n",
    "\n",
    "import cv2, os, time\n",
    "\n",
    "# âœ… Change this to the gesture you want to record\n",
    "gesture = \"Water\"  \n",
    "\n",
    "BASE_DIR = \"dataset\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_DIR, gesture), exist_ok=True)\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 20.0\n",
    "\n",
    "# Count existing recordings\n",
    "count = len(os.listdir(os.path.join(BASE_DIR, gesture)))\n",
    "print(f\"Recording gesture: {gesture}\")\n",
    "print(\"Press 'r' to start recording a ~2s clip. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    cv2.imshow(\"Single Gesture Recorder\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    \n",
    "    # Press 'r' to record\n",
    "    if key == ord('r'):\n",
    "        out_name = f\"{gesture}_{count}.mp4\"\n",
    "        out_path = os.path.join(BASE_DIR, gesture, out_name)\n",
    "        h, w = frame.shape[:2]\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(out_path, fourcc, 20.0, (w, h))\n",
    "        print(f\"Recording {gesture} -> {out_path}\")\n",
    "        start = time.time()\n",
    "        # Record ~2 seconds\n",
    "        while time.time() - start < 2.0:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            writer.write(frame)\n",
    "            cv2.imshow(\"Single Gesture Recorder\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        writer.release()\n",
    "        count += 1\n",
    "        print(f\"Saved {out_path}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, numpy as np, cv2, mediapipe as mp\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "BASE=\"dataset\"\n",
    "OUT=\"processed/keypoints\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "SEQ_LEN = 20  # smaller seq_len for CPU\n",
    "\n",
    "def extract_keypoints_from_video(path, seq_len=SEQ_LEN):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "    seq=[]\n",
    "    while len(seq) < seq_len:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = hands.process(rgb)\n",
    "        if res.multi_hand_landmarks:\n",
    "            data=[]\n",
    "            for hand in res.multi_hand_landmarks:\n",
    "                for lm in hand.landmark:\n",
    "                    data += [lm.x, lm.y, lm.z]\n",
    "            if len(res.multi_hand_landmarks)==1:\n",
    "                data += [0]*63\n",
    "            seq.append(data)\n",
    "        else:\n",
    "            seq.append([0]*126)\n",
    "    while len(seq) < seq_len:\n",
    "        seq.append([0]*126)\n",
    "    cap.release(); hands.close()\n",
    "    return np.array(seq, dtype=np.float32)\n",
    "\n",
    "for gesture in sorted(os.listdir(BASE)):\n",
    "    in_dir = os.path.join(BASE, gesture)\n",
    "    out_dir = os.path.join(OUT, gesture)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for mp4 in glob.glob(os.path.join(in_dir,\"*.mp4\")):\n",
    "        name = os.path.splitext(os.path.basename(mp4))[0]\n",
    "        out_path = os.path.join(out_dir, name + \".npy\")\n",
    "        if os.path.exists(out_path): continue\n",
    "        arr = extract_keypoints_from_video(mp4)\n",
    "        np.save(out_path, arr)\n",
    "        print(\"Saved\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "BASE=\"dataset\"\n",
    "OUT=\"processed/frames\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "TARGET_FRAMES = 8    # fewer frames\n",
    "TARGET_SIZE = (64,64)  # smaller resolution\n",
    "\n",
    "def extract_video_clip(path, target_frames=TARGET_FRAMES, size=TARGET_SIZE):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames=[]\n",
    "    while len(frames) < target_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        frame = cv2.resize(frame, size)\n",
    "        frames.append(frame[..., ::-1])\n",
    "    cap.release()\n",
    "    while len(frames) < target_frames:\n",
    "        frames.append(frames[-1] if frames else np.zeros((size[1],size[0],3), dtype=np.uint8))\n",
    "    arr = np.stack(frames, axis=0).astype(np.uint8)\n",
    "    return arr\n",
    "\n",
    "for gesture in sorted(os.listdir(BASE)):\n",
    "    in_dir = os.path.join(BASE,gesture)\n",
    "    out_dir = os.path.join(OUT,gesture)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for mp4 in glob.glob(os.path.join(in_dir,\"*.mp4\")):\n",
    "        name = os.path.splitext(os.path.basename(mp4))[0]\n",
    "        out_path = os.path.join(out_dir, name + \".npy\")\n",
    "        if os.path.exists(out_path): continue\n",
    "        clip = extract_video_clip(mp4)\n",
    "        np.save(out_path, clip)\n",
    "        print(\"Saved frames\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, os, glob\n",
    "\n",
    "device = torch.device(\"cpu\")  # CPU only\n",
    "\n",
    "# Keypoint dataset\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, root=\"processed/keypoints\"):\n",
    "        self.samples=[]; self.labels=[]\n",
    "        self.classes = sorted(os.listdir(root))\n",
    "        for i,cls in enumerate(self.classes):\n",
    "            for npy in glob.glob(os.path.join(root,cls,\"*.npy\")):\n",
    "                self.samples.append(npy)\n",
    "                self.labels.append(i)\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        arr = np.load(self.samples[idx])\n",
    "        return torch.tensor(arr, dtype=torch.float32), self.labels[idx]\n",
    "\n",
    "# Frame dataset\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root=\"processed/frames\", seq_len=12, resize=(32,32)):\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.seq_len = seq_len\n",
    "        self.resize = resize\n",
    "        self.classes = sorted(os.listdir(root))\n",
    "        self.class2idx = {c:i for i,c in enumerate(self.classes)}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(root, cls)\n",
    "            for npy in glob.glob(os.path.join(cls_path, \"*.npy\")):\n",
    "                self.samples.append(npy)\n",
    "                self.labels.append(self.class2idx[cls])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        arr = np.load(self.samples[idx]).astype(np.float32)/255.0  # normalize\n",
    "        # Pad or crop sequence to seq_len\n",
    "        T, H, W, C = arr.shape\n",
    "        if T < self.seq_len:\n",
    "            pad = np.zeros((self.seq_len - T, H, W, C), dtype=np.float32)\n",
    "            arr = np.concatenate([arr, pad], axis=0)\n",
    "        elif T > self.seq_len:\n",
    "            arr = arr[:self.seq_len]\n",
    "\n",
    "        # Resize frames if needed\n",
    "        if self.resize:\n",
    "            arr_resized = np.stack([cv2.resize(f, self.resize) for f in arr], axis=0)\n",
    "            arr = arr_resized\n",
    "\n",
    "        # Transpose to [C, T, H, W] for Conv3D\n",
    "        arr = np.transpose(arr, (3,0,1,2))\n",
    "        return torch.tensor(arr, dtype=torch.float32), self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GestureLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=126, hidden_dim=64, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "    def forward(self,x):\n",
    "        out,_ = self.lstm(x)\n",
    "        return self.fc(out[:,-1,:])\n",
    "\n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv3d(3,16,3,padding=1), nn.ReLU(), nn.MaxPool3d((1,2,2)),\n",
    "            nn.Conv3d(16,32,3,padding=1), nn.ReLU(), nn.MaxPool3d((2,2,2)),\n",
    "            nn.Conv3d(32,64,3,padding=1), nn.ReLU(), nn.AdaptiveAvgPool3d((1,1,1)),\n",
    "        )\n",
    "        self.fc = nn.Linear(64,num_classes)\n",
    "    def forward(self,x):\n",
    "        x=self.net(x)\n",
    "        x=x.view(x.size(0),-1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace with your file path\n",
    "file_path = \"processed/keypoints/I/I_0.npy\"\n",
    "\n",
    "data = np.load(file_path)\n",
    "print(data.shape)   # Check the shape of the array\n",
    "print(data)         # See the actual data\n",
    "print(\"Sequence length:\", data.shape[0])\n",
    "print(\"Number of features per frame:\", data.shape[1])\n",
    "print(\"First frame keypoints:\", data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "kp_dataset = KeypointDataset(\"processed/keypoints\")\n",
    "kp_loader = DataLoader(kp_dataset, batch_size=4, shuffle=True)  # small batch\n",
    "\n",
    "model_kp = GestureLSTM(input_dim=kp_dataset[0][0].shape[1], hidden_dim=64, num_classes=len(kp_dataset.classes)).to(device)\n",
    "opt = torch.optim.Adam(model_kp.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for ep in range(50):  # fewer epochs for CPU\n",
    "    total_loss=0; cnt=0\n",
    "    model_kp.train()\n",
    "    for X,y in kp_loader:\n",
    "        X=X.to(device); y=torch.tensor(y).to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model_kp(X)\n",
    "        loss = crit(out,y)\n",
    "        loss.backward(); opt.step()\n",
    "        total_loss+=loss.item(); cnt+=1\n",
    "    print(f\"Epoch {ep+1}, loss={total_loss/cnt:.4f}\")\n",
    "torch.save(model_kp.state_dict(),\"gesture_lstm_cpu.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# --- Dataset ---\n",
    "# Make sure your VideoDataset class accepts seq_len and resize\n",
    "video_ds = VideoDataset(\"processed/frames\", seq_len=12, resize=(32,32))\n",
    "video_loader = DataLoader(video_ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# --- Model ---\n",
    "model_3d = Simple3DCNN(num_classes=len(video_ds.classes)).to(device)\n",
    "opt3 = torch.optim.Adam(model_3d.parameters(), lr=1e-4)\n",
    "crit3 = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Training parameters ---\n",
    "num_epochs = 50  # can increase further for better accuracy\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    model_3d.train()\n",
    "    \n",
    "    for X, y in video_loader:\n",
    "        # Ensure correct tensor type\n",
    "        X = X.float().to(device)       # [batch, C, seq_len, H, W]\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        \n",
    "        opt3.zero_grad()\n",
    "        out = model_3d(X)\n",
    "        loss = crit3(out, y)\n",
    "        loss.backward()\n",
    "        opt3.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        cnt += 1\n",
    "    \n",
    "    print(f\"3D Epoch {ep+1}/{num_epochs}, loss={total_loss/cnt:.4f}\")\n",
    "\n",
    "# --- Save model ---\n",
    "MODEL_SAVE_PATH = \"gesture_3d_cpu.pth\"\n",
    "torch.save(model_3d.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Saved improved 3D-CNN model to {MODEL_SAVE_PATH}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "def eval_model(model, dataset, batch_size=1):\n",
    "    model.eval()\n",
    "    preds=[]; trues=[]\n",
    "    for X,y in DataLoader(dataset,batch_size=batch_size):\n",
    "        X=X.to(device)\n",
    "        with torch.no_grad(): out = model(X)\n",
    "        preds += out.argmax(1).cpu().numpy().tolist()\n",
    "        trues += y\n",
    "    return accuracy_score(trues,preds)\n",
    "\n",
    "acc_kp = eval_model(model_kp,kp_dataset)\n",
    "acc_3d = eval_model(model_3d,video_ds)\n",
    "print(\"LSTM CPU acc:\",acc_kp)\n",
    "print(\"3D-CNN CPU acc:\",acc_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Water\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: I\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 2, Predicted gesture: Thank You\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Food\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n",
      "[DEBUG] Hands detected: 1, Predicted gesture: Sorry\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import cv2, mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pyttsx3\n",
    "\n",
    "# -----------------------\n",
    "# Real-time LSTM Gesture Recognition (CPU)\n",
    "# -----------------------\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "SEQ_LEN = 20  # should match training\n",
    "GESTURES = [\"Food\", \"I\", \"Sorry\", \"Thank You\", \"Water\"]  # same as training\n",
    "device = torch.device(\"cpu\")\n",
    "MODEL_PATH = \"gesture_lstm_cpu.pth\"\n",
    "\n",
    "# Function to load model\n",
    "def load_lstm_model():\n",
    "    model = GestureLSTM(input_dim=126, hidden_dim=64, num_classes=len(GESTURES)).to(device)\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        model.eval()\n",
    "        print(\"Loaded trained model from\", MODEL_PATH)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{MODEL_PATH} not found. Train model first.\")\n",
    "    return model\n",
    "\n",
    "# Load model once\n",
    "model_kp = load_lstm_model()\n",
    "\n",
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.3,  # lower threshold\n",
    "    min_tracking_confidence=0.3\n",
    ")\n",
    "buf = collections.deque(maxlen=SEQ_LEN)\n",
    "\n",
    "prev_time = 0  # for FPS calculation\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    res = hands.process(rgb)\n",
    "    \n",
    "    # Extract keypoints for LSTM\n",
    "    data = []\n",
    "    hands_detected = 0\n",
    "    if res.multi_hand_landmarks:\n",
    "        hands_detected = len(res.multi_hand_landmarks)\n",
    "        for i, hand in enumerate(res.multi_hand_landmarks):\n",
    "            if i >= 2:\n",
    "                break\n",
    "            for lm in hand.landmark:\n",
    "                data += [lm.x, lm.y, lm.z]\n",
    "        while len(data) < 126:\n",
    "            data += [0]\n",
    "        for hand_landmarks in res.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    else:\n",
    "        data = [0]*126\n",
    "\n",
    "    buf.append(np.array(data, dtype=np.float32))\n",
    "\n",
    "    # Show buffer status\n",
    "    cv2.putText(frame, f\"Buffer: {len(buf)}/{SEQ_LEN}\", (10, 110),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "    # Show warning if no hands\n",
    "    if hands_detected == 0:\n",
    "        cv2.putText(frame, \"No hands detected!\", (10, 140),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # Predict gesture only if at least one hand is detected\n",
    "    if len(buf) == SEQ_LEN and hands_detected > 0:\n",
    "        seq_input = torch.tensor([list(buf)], dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model_kp(seq_input)\n",
    "            pred = out.argmax(dim=1).item()\n",
    "        gesture_word = GESTURES[pred]    \n",
    "        cv2.putText(frame, f\"Gesture: {GESTURES[pred]}\", (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        print(f\"[DEBUG] Hands detected: {hands_detected}, Predicted gesture: {GESTURES[pred]}\")\n",
    "        tts_engine.say(gesture_word)\n",
    "        tts_engine.runAndWait()\n",
    "\n",
    "    # Calculate and display FPS\n",
    "    curr_time = time.time()\n",
    "    fps = 1 / (curr_time - prev_time) if prev_time else 0\n",
    "    prev_time = curr_time\n",
    "    cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 80),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Real-time Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "# ...existing code...\n",
    "model_3d = Simple3DCNN(num_classes=len(GESTURES)).to(device)\n",
    "model_3d.load_state_dict(torch.load(\"gesture_3d_cpu.pth\", map_location=device))\n",
    "model_3d.eval()\n",
    "print(\"Loaded 3D-CNN model successfully!\")\n",
    "\n",
    "tts_engine = pyttsx3.init()  # Add TTS engine\n",
    "\n",
    "SEQ_LEN = 16  # match what was used in training\n",
    "frame_buffer = deque(maxlen=SEQ_LEN)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize to 64x64 (or your training size)\n",
    "    frame_resized = cv2.resize(frame, (64,64))\n",
    "    frame_buffer.append(frame_resized)\n",
    "\n",
    "    # Predict when buffer is full\n",
    "    if len(frame_buffer) == SEQ_LEN:\n",
    "        # Convert buffer to tensor: [batch=1, C=3, seq, H, W]\n",
    "        frames_np = np.stack(frame_buffer, axis=0)       # [seq, H, W, C]\n",
    "        frames_np = frames_np.transpose(3,0,1,2)        # [C, seq, H, W]\n",
    "        frames_tensor = torch.tensor(frames_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        frames_tensor /= 255.0  # normalize if model trained on [0,1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model_3d(frames_tensor)\n",
    "            pred = out.argmax(dim=1).item()\n",
    "        gesture_word = GESTURES[pred]\n",
    "        cv2.putText(frame, f\"3D-CNN Gesture: {gesture_word}\", (10,40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "        tts_engine.say(gesture_word)\n",
    "        tts_engine.runAndWait()\n",
    "\n",
    "    cv2.imshow(\"3D-CNN Gesture Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
